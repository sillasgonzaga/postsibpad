---
title: "Utilizando Machine Learning para prever se uma reclamação no Consumidor.gov.br será atendida: como saber qual é o melhor modelo"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document:
    keep_md: yes
  github_document: default
  
---

## Introdução

Em Ciência de Dados, é comum direcionar todos os esforços nos Dados e deixar a Ciência de lado. Com a popularização de ferramentas como R e Python, que tornaram fácil o ajuste e aplicação de modelos complexos de Machine Learning para qualquer um que saiba ler e acompanhar um texto em um blog, é natural que os praticantes de Data Science sejam ansiosos a ponto de sair tentando modelar qualquer tipo de conjunto de dados, desrespeitando a etapa fundamental de dar um passo para trás e pensar no que está fazendo. Quais os pressupostos dos modelos usados e o que eles significam? O que um valor ausente representa para uma variável? O que pode ser considerado um outlier e qual o tratamento que deve ser dado a ele? O modelo serve para ser colocado em produção? Como avaliar a estabilidade de seus resultados?

Muitos outros questionamentos são possíveis e devem ser explorados em qualquer projeto de Data Science. Neste post, abordamos um desses, essencial para a escolha de um modelo preditivo em meio a outros "concorrentes": como avaliar a qualidade dos resultados de um modelo?

```{r pacotes, message = FALSE, warning=FALSE}
library(tidyverse)
library(janitor)
library(magrittr)
library(abjutils)
library(caret)
library(formattable)
```


```{r}
# Refazer limpeza dos dados
df <- read.csv2("data/2017-07.csv", fileEncoding =  "ISO-8859-1", stringsAsFactors = FALSE)
# limpar nomes das colunas
df %<>% clean_names()
# remover acentos das colunas
names(df) %<>% rm_accent()
df %<>% filter(avaliacao_reclamacao != "Não Avaliada")

set.seed(123) 
ind <- sample(1:(nrow(df)*0.7), replace = FALSE)
train <- df[ind,]
test <- df[-ind,]
```


## Sobre data leakage

Data leakage é um dos 10 maiores erros em Data Mining e se refere ao processo de introduzir informações sobre a variável resposta do modelo que não deveriam estar disponíveis. Por exemplo, usar o número da conta em um problema de prever se um cliente em potencial de um banco iria abrir ou não um conta. Obviamente, o número da conta indica que o indivíduo já abriu a conta.

No nosso caso, o que teria acontecido se tivéssemos usado a variável `nota_do_consumidor` como regressor do modelo?

```{r}
table(df$nota_do_consumidor, df$avaliacao_reclamacao)
```

Notas mais altas (a partir de 3) estão mais associadas com reclamações resolvidas. Além de isso ser uma obviedade, é provável que o consumidor dê sua avaliação numérica do atendimento apenas após o consenso. Afinal, como uma reclamação poderá ser avaliada de 1 a 5 se ela não foi finalizada? Para uma boa referência sobre Data Leakage, leia [este artigo de 8 páginas](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf).


## Matriz de confusão

Em um problema de classificação binário, como é o nosso caso de estudo, define-se uma das classes de um indivíduo (como uma reclamação no Consumidor.gov.br) como positiva ou negativa. Como é mais importante que saibamos com antecedência se uma reclamação será não-atendida do que atendida (para poder auxiliar na sua resolução, por exemplo), trataremos as Não Resolvida como positivo e Resolvida como negativo.

A eficácia de um algoritmo de classificação pode ser avaliada contando o número de indíviduos corretamente classificados como positivos (**verdadeiros positivos**), os corretamente classificados como negativos (**verdadeiros negativos**) e os exemplos que ou foram incorretamente classificados como pertencentes à classe (**falsos positivos**) ou os que são positivos mas foram previstos como negativos (**falsos negativos**). A representação desses quatro valores é feita por uma matriz de confusão, que no R é implementada pela função `caret::confusionMatrix()`:

```{r}
# carregar previsoes
previsoes <- readRDS("data/previsoes-consumidor.gov.Rds")

# exemplo para a arvore de decisao
confusionMatrix(data = previsoes$decision.tree, reference = test$avaliacao_reclamacao,
                positive = "Não Resolvida", mode = "everything")
```

Diversas métricas de acurácia são mostradas. Algumas delas são:

* **Sensitivity**: De todos as reclamações não-resolvidas, quantas foram previstas corretamente? 17,9%;  
* **Specificity**: De todos as reclamações resolvidas, quantas foram previstas corretamente? 90,3%;  
* **Accuracy**: De todos as reclamações, quantas foram previstas corretamente? 63,5%;  
* **Precision**: De todos as reclamacṍes previstas como não-resolvidas, quantas de fato não foram resolvidas? 52,1%;  
* **Detection rate**: De todas as reclamações, quantas foram classificados corretamente em não-resolvidas? 6,6%;  
* **No Information Rate**: Também conhecido como *Null Error Rate*. Qual teria sido a acurácia de um modelo que previsse sempre que uma reclamação seria atendida? 63%;  

Pense agora no nosso caso: qual dessas métricas seria a mais importante? Depende muito do ponto de vista e da interpretação. Pessoalmente, acredito que a métrica de **Precision** seja a mais relevante, pois, como  o objetivo da modelagem é dar mais atenção às reclamações que supostamente não serão resolvidas, é necessário que essa classificação seja a mais precisa por isso. Afinal de contas, não queremos dar trabalho extra para os analistas do Consumidor.gov.br

## Estudo de caso

Extrair as métricas de acurácia para todos os modelos ajustados no post anterior também é simples:

```{r}
df.stats <- previsoes %>%
  # aplicar matriz de confusao para todos os modelos
  map(confusionMatrix, reference = test$avaliacao_reclamacao, positive = "Não Resolvida") %>% 
  # extrair metricas
  map("byClass") %>% 
  as.data.frame() %>% 
  rownames_to_column("metrica") %>% 
  # arredondar colunas numericas
  mutate_if(is.numeric, round, 3)


# usar pacote formattable para colorir linhas de acordo com seu valor
formattable(df.stats, 
            lapply(1:nrow(df.stats), function(row){
                area(row, col = -1) ~ color_tile("red", "green")
            }))


```

Essa tabela ilustra bem a importância de se definir bem qual a métrica de acurácia que usaremos. Em quase todas as métricas, o melhor modelo é o Naive Bayes, com exceção justamente da que definimos como a mais importante: O melhor modelo em precision é o randomForest.

Também é possível mostrar esses resultados em um gráfico:

```{r, fig.width = 9}

df.stats %>% 
  # converter para formato long para ggplot2
  gather(modelo, valor, -metrica) %>% 
  filter(! metrica %in% c("Prevalence", "Recall")) %>% 
  ggplot(aes(x = modelo, y = valor)) + 
    geom_col() +
    coord_flip() +
    facet_wrap( ~ metrica, scales = 'free', nrow = 3) +
    labs(y = "Resultado", x = "Modelo de classificação",
         title = "Ranking de acurácia de modelos de classificação de acordo com diferentes métricas")
 
```

O gráfico nos ajuda a perceber que na maioria das métricas todos os modelos têm resultados muito parecidos entre si, com exceção de algumas em que o modelo Naive Bayes abriu uma diferença razoável em relação aos demais.

## Comentários finais

Apesar de ter feito com uma boa dose de carinho e um nível de pesquisa acadêmico que não costumo fazer para posts de blogs, é válido ressaltar que esses 4 posts não substituem de maneira alguma um estudo completo de algoritmos de classificação. O processo de modelagem foi razoavelmente ingênuo e não se buscou variáveis externas que poderiam aumentar a performance dos modelos. 

## Referências

[Top 10 data mining mistakes - PDF](https://pdfs.semanticscholar.org/d7cc/422734103b32e1296ca4a06d3e59cacc72dd.pdf)  
[Beyond Accuracy, F-score and ROC: a Family of Discriminant Measures for Performance Evaluation - PDF](https://vvvvw.aaai.org/Papers/Workshops/2006/WS-06-06/WS06-06-006.pdf)

